{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_4563336072139954371() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_4563336072139954371()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Methods ###\n",
    "\n",
    "In this notebook we will examine and implement various policy gradient methods for reinforcement learning.\n",
    "\n",
    "Credits go to Seita's blog, John Schulman's notes and PhD thesis, and the CS234 lectures, from which the explanations in this notebook are synthesized. I used the CS234 assignment 3 startercode to focus more on the conceptual parts of implementing the learning algorithms over implementation parts like logging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient ###\n",
    "\n",
    "We will first implement vanilla policy gradient (REINFORCE), a classic policy gradient reinforcement learning algorithm.\n",
    "\n",
    "The goal of reinforcement learning is for an agent to learn to act in a dynamic environment so as to maximize its expected cumulative reward over the course of a time-horizon. Policy gradient methods solve the problem of control by directly learning the policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ from observations of rewards obtained by interacting with the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally define a trajectory $\\tau$ as a tuple $(s_0, a_0, r_0, s_1, a_1, ..., r_T)$ denoting a sequence of state-action-rewards observed over the course of some episode of interaction with the agent's environment, and let $R(\\tau)$ denote the finite-horizon return, aka cumulative sum of rewards. Then our goal is the maximize the _expected_ finite-horizon return where the expectation is over trajectories sampled from the stochastic policy $\\pi_{\\theta}$ (here we let $\\theta$ denote the parameters of the policy $\\pi$)--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "max_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize $J$ using gradient ascent, we need an efficiently computable form of its gradient. First, let's compute an analytical form of the gradient, then see how we can approximate it with sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-derivative trick to push a form of the gradient of the policy into the the gradient of $J$.\n",
    "\n",
    "I liked Seita's explanation of it--\"the log derivative trick tells us how to insert a log into an expectation when starting from $\\nabla_{\\theta} \\mathbb{E}[f(x)]$\"\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}[f(x)] &= \\nabla_{\\theta} \\int p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int \\dfrac{p_{\\theta}(x)}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int p_{\\theta}(x) \\nabla_{\\theta} \\log p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\mathbb{E}[\\nabla_{\\theta} \\log p_{\\theta}(x) f(x)]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gradient of $J$ we are concerned with the gradient of the log probability of trajectories, so we derive its form now.\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(\\tau) &= \\nabla_{\\theta} \\log \\left( \\mu(s_{0}) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t|s_t) P(s_{t+1|s_t,a_t}) \\right) \\\\\n",
    "&= \\nabla_{\\theta} \\left( \\log \\mu_{s_0} + \\sum_{t=0}^{T-1} (\\log \\pi_{\\theta}(a_t | s_t) + \\log P(s_{t+1} | s_t, a_t))\\right) \\\\\n",
    "&= \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the pieces together, the gradient of the expected return is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log probability of each action is weighted by the rewards associated with it. The gradient has an intuitive interpretation--it encourages us to increase the probability of actions which lead to high expected return.\n",
    "\n",
    "Conveniently, $\\nabla_{\\theta} J$ turns out to have the form of an expectation. Because of this, we can estimate it a la Monte Carlo methods using samples from our environment--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{g} = \\dfrac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal{D}$ is a dataset of trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance ###\n",
    "\n",
    "An issue with the estimator $\\hat{g}$ is that it has high variance. (One of the reasons for this, which we will go into more depth later, is that $R(\\tau)$ sums up individual reward samples over long sequences.) If we have high variance in our estimate of the policy gradient, then the update will cause the policy to fluctuate across updates in ways that don't reflect the overall dynamics of the environment.\n",
    "\n",
    "One way to decrease variance of the estimator starts by observing that the action taken at time $t$ affects only the rewards reaped at timestep $t$ onwards. This makes sense in terms of credit assignment, and also reduces the number of timesteps we sum over. Let's do some algebra to get the gradient into a form we want\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R (\\tau)] &= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t' = 0}^T r_{t'} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})]  \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert explanation here for algebra]\n",
    "\n",
    "The literature likes to refer to $\\sum_{t' = t}^T r_{t'}$ as rewards-to-go. We weight each action its causal consequence--the future rewards it impacts. We note that this form has a resemblance to Bellman forms of value, which concern the reward at a given time step, as well as the future consequences.\n",
    "\n",
    "A second way to decrease the variance of the policy gradient estimate is by subtracting a baseline from the rewards-to-go. \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'} - b(s_t))]\n",
    "\\end{equation}\n",
    "\n",
    "An intuitive choice for $b(s_t)$ is the state-value function $V^{\\pi}(s_t)$. When we use the state-value function as a baseline, we are choosing to weight an action by the difference between the rewards we got in our sample and the rewards we expected to get. If the difference is close to zero, then we shouldn't need to change our policy too much--i.e. the gradient for that state-action should be close to zero in the update.\n",
    "\n",
    "There are a few ways to more formally explain why the baseline reduces the variance of the policy gradient estimator. I'll first summarize Seita's explanation, which is an approximate one.\n",
    "\n",
    "\\begin{align*}\n",
    "Var(\\hat{g}) &= Var(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t)) \\\\\n",
    "&  \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t))^2] \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[(\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He first approximates the variance of a sum by the sum over timesteps of variances. Then he uses the formula $Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ noting that $\\mathbb{E}[X] = 0$ for the log probability form. [To-do: explanation and proof]. The third approximation comes from independence. Seita says that the approximations are fine since recent advances in RL, like A3C, break up correlation among samples.\n",
    "\n",
    "Based on these approximations, we can see that that a well-chosen baseline can decrease the variance of $\\hat{g}$.\n",
    "\n",
    "I'm unsatisfied with these explanations, and plan on adding my own based off of the CS234 assignment to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network ###\n",
    "\n",
    "The policy is represented as a multi-layer perceptron so that we can learn to act in environments with high dimensional states and/or continous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, size):    \n",
    "    modules = OrderedDict()\n",
    "    modules['Linear_Input'] = nn.Linear(input_size, size)\n",
    "    modules['ReLU_Input'] = nn.ReLU()\n",
    "    for i in range(n_layers):\n",
    "        modules['Linear_'+str(i)] = nn.Linear(size, size)\n",
    "        modules['ReLU_'+str(i)] = nn.ReLU()\n",
    "    modules['Linear_Output'] = nn.Linear(size,output_size)\n",
    "    sequential = nn.Sequential(modules)\n",
    "    return sequential\n",
    "\n",
    "# cartpole\n",
    "observation_dim        = 4\n",
    "n_layers               = 1\n",
    "layer_size             = 64\n",
    "\n",
    "policy_network = build_mlp(observation_dim, 1, n_layers, layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss ###\n",
    "\n",
    "Below is the implementation of the policy gradient update. Note here that advantage denotes $\\sum_{t' = t}^T r_{t'}- V^{\\pi}(s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_8284323280569314814() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_8284323280569314814()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.chdir(\"/Users/annhe/vpg/code\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code/policy_gradient.py\n",
    "\n",
    "def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        actions = np2torch(actions)\n",
    "        advantages = np2torch(advantages)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        res = self.policy.action_distribution(observations).log_prob(actions) \n",
    "\n",
    "        loss = -(res * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Refitting ###\n",
    "\n",
    "Note that we will have to update the baseline, $V^{\\pi}(s_t)$ every time we perform a policy update. This is because we want our estimate of the value function to be as accurate as possible, given the data we have. To do so, we compute the mean squared error loss between a forward pass of the network (current value estimate) and the most recent returns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size], containing all discounted\n",
    "                future returns for each step\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        \"\"\"\n",
    "        returns = np2torch(returns)\n",
    "        observations = np2torch(observations)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.forward(observations)\n",
    "        loss = ((returns - preds)**2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###\n",
    "\n",
    "Now we are ready to run some experiments. We would hope to see that the introduction of the baseline improves performance over just using the returns in the policy gradient estimate. To do so, we experiment on three OpenAI gym environments--cartpole, pendulum, and half-cheetah. For each environment we choose three random seeds, and run the agent with and without baseline using those three seeds. We plot the average expected return for each experiment with error bars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cartpole and pendulum results do not really demonstrate the difference between baseline and no baseline, but cheetah seems to be a complex enough environment to show the difference. In cheetah, we see that the baseline approach outperforms no baseline, especially at the very end, where the no baseline approach actual deteriorates in performance while the baseline approach continues to improve. Without a baseline, the policy gradient updates may be too large. In fact, even with a baseline, VPG is too \"greedy\" an approach. An intuitive explanation is that overcorrecting (changing the way you act too drastically) based on feedback may incur immediate benefits but prevent long term learning and generalization. Follow-up work like TRPO and PPO aim to make monotonic improvements in performance by constraining the difference between the policy before and after updates using KL-Divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Results / Baseline vs no Baseline ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole-old.png\">\n",
    "<img src=\"code/results/results-pendulum.png\">\n",
    "<img src=\"code/results/results-cheetah.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Improvement and TRPO ###\n",
    "\n",
    "As shown in the experiment above, VPG does not necessarily make monotonic improvements to the expected performance of a policy. TRPO constrains the step-size of the gradient updates to impose monotonic improvement. First we derive the theoretical basis for TRPO, then we look into the details and make approximations and estimates in order to implement it.\n",
    "\n",
    "Instead of setting the optimization objective to be the expected performance of a policy, we instead set the objective to be a loss function whose monotonic improvement guaranteed monotonic improve of the policy itself. Consider the following identity due to Kakade and Langford,\n",
    "\n",
    "\\begin{equation}\n",
    "\\eta_{\\pi}(\\tilde{\\pi}) = \\eta_{\\pi}(\\pi) + \\mathbb{E}_{\\tau \\sim \\tilde{\\pi}}[\\sum_{t=0}^{\\infty} \\gamma^{t} A_{\\pi}(s_{t}, a_{t})]\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\eta_{\\pi}(\\pi)$ is the discounted sum of rewards from following $\\pi$ and $A_{\\pi}(s_{t}, a_{t})$ is the advantage of taking action $a_{t}$ over following $\\pi$ at time step $t$. If we massage this equation into a slightly different form, we can begin to see the theoretical basis for TRPO.\n",
    "\n",
    "Note that $\\mathbb{E}_{\\tau \\sim \\tilde{\\pi}} = \\mathbb{E}_{s_0, a_0, s_1, a_1, ... \\sim \\tilde{\\pi}}$.\n",
    "\n",
    "If we define $P(s_t = s | \\tilde{\\pi})$ to be the probability that we end up in state $s$ at time $t$ by following $\\pi$, then we can conveniently rewrite $\\eta_{\\pi}(\\tilde{\\pi})$ as \n",
    "\n",
    "\\begin{align*}\n",
    "\\eta_{\\pi}(\\tilde{\\pi}) &= \\eta_{\\pi}(\\pi) + \\mathbb{E}_{s_0, a_0, s_1, a_1, ... \\sim \\tilde{\\pi}}[\\sum_{t=0}^{\\infty} \\gamma^{t} A_{\\pi}(s_{t}, a_{t})] \\\\\n",
    "&= \\eta_{\\pi}(\\pi) + \\sum_{t=0}^{\\infty} \\sum_{s} P(s_t = s | \\tilde{\\pi}) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a) \\\\\n",
    "&= \\eta_{\\pi}(\\pi) +  \\sum_{s} \\sum_{t=0}^{\\infty} P(s_t = s | \\tilde{\\pi}) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a) \\\\\n",
    "&= \\eta_{\\pi}(\\pi) +  \\sum_{s} p_{\\tilde{\\pi}}(s) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "Where $p_{\\tilde{\\pi}}(s) = P(s_0 = s | \\tilde{\\pi}) + \\gamma \\cdot P(s_1 = s | \\tilde{\\pi}) + ...$ is the discounted state visitation frequency. From the formulation above, we can see that any policy update that has positive expected advantage at every state is guaranteed to improve policy performance.\n",
    "\n",
    "However, in our computation of the policy update, we cannot compute $p_{\\tilde{\\pi}}(s)$ as we do not have samples from the new policy $\\tilde{\\pi}$. We make a local approximation to $\\eta$ by using $p_{\\pi}(s)$ instead, yielding the modified objective\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\pi}(\\tilde{\\pi}) = \\eta_{\\pi}(\\pi) +  \\sum_{s} p_{\\pi}(s) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "The first order gradient of $L_{\\pi}(\\tilde{\\pi})$ and $\\eta_{\\pi}(\\tilde{\\pi})$ are equivalent, which means we can indirectly improve $\\eta_{\\pi}(\\tilde{\\pi})$ by directly optimizing $L_{\\pi}(\\tilde{\\pi})$. However, we do not yet know how large a step size we can take and still have guaranteed improvement.\n",
    "\n",
    "A theorem (cite) says\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi_{new}} \\ge L_{\\pi_{old}}(\\pi_{new}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} \\left( D_{TV}^{max}(\\pi_{old}, \\pi_{new}) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Since $(D_{TV})^2$ is upper bounded by KL divergence, we also get\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi_{new}} \\ge L_{\\pi_{old}}(\\pi_{new}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{old}, \\pi_{new}) \n",
    "\\end{equation}\n",
    "\n",
    "To see that we can get monotonic improvement by optimizing $L$,\n",
    "\n",
    "Let $M_i(\\pi) = L_{\\pi_i}(\\pi) - \\dfrac{4 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{old}, \\pi_{new})$\n",
    "\n",
    "Note that $M_i(\\pi_i) = L_{\\pi_i}(\\pi_i) =  V^{\\pi_i}$\n",
    "\n",
    "Then \n",
    "\n",
    "\\begin{align*}\n",
    "V^{\\pi_{i+1}} & \\ge L_{\\pi_{i}}(\\pi_{i+1}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{i}, \\pi_{i+1}) \\\\\n",
    "\\end{align*}\n",
    "And subtracting $V^{\\pi_{i}}$ from both sides\n",
    "\\begin{align*}\n",
    "V^{\\pi_{i+1}} - V^{\\pi_{i}} & \\ge M_{i}(\\pi_{i+1}) - M_{i}(\\pi_{i})\\\\\n",
    "\\end{align*}\n",
    "Since $M_i(\\pi_i) =  V^{\\pi_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of optimizing $M$, which includes a penalty term for the KL divergence, TRPO optimizes against a constraint on the KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample-Based Estimation of the Objective and Constraint ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO vs VPG (baseline) ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole-ppo-vpg.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
