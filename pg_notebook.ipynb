{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_4563336072139954371() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_4563336072139954371()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and Implementing Policy Gradient Methods ###\n",
    "\n",
    "Policy gradients are a pretty cool class of reinforcement learning algorithms. They focus on directly optimizing the metric we care most about in reinforcement learning (the expected reward from acting in an environment), and because of this enjoy an elegant formulation that looks very similar to supervised maching learning, and has stability benefits over approaches like Q-learning, which indirectly learn a policy, and can suffer from too much approximation \\[link to deadly triad\\].\n",
    "\n",
    "While this blog post is in no sense comprehensive, I hope to show a good mixture of theory, what these algorithms look like in python code, emprical results, and high-level questions. There's a lot a great blog posts out there on policy gradients, and many go into depth in the mathematical fundamentals or empirical observations. I hope to (a) provide enough technical detail to serve as a tutorial and (b) roughly show the line of thought that a scientist might have that would lead the development from simple Vanilla Policy Gradient, to the more advanced Trust Region Policy Optimization, and then to Proximal Policy Optimization, which is conceptually similar to TRPO but much easier to implement, and thus a popular sota baseline. \n",
    "\n",
    "Credits go to Daniel Takeshi's blog, John Schulman's PhD thesis, OpenAI's Spinning Up, and the Stanford CS234 lectures, from which the explanations in this post are synthesized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient ###\n",
    "\n",
    "Let's first implement vanilla policy gradient (REINFORCE), the backbone of these three methods. (My acknowledgements go to Daniel Takeshi, since this section is highly based off of his blog post.)\n",
    "\n",
    "The goal of reinforcement learning is for an agent to learn to act in a dynamic environment so as to maximize its expected cumulative reward over the course of a time-horizon. Policy gradient methods solve the problem of control by directly learning the policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ from observations of rewards obtained by interacting with the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally define a trajectory $\\tau$ as a tuple $(s_0, a_0, r_0, s_1, a_1, ..., r_T)$ denoting a sequence of state-action-rewards observed over the course of some episode of interaction with the agent's environment, and let $R(\\tau)$ denote the finite-horizon return, aka cumulative sum of rewards over finite timesteps. Then our goal is the maximize the _expected_ finite-horizon return where the expectation is over trajectories sampled from the stochastic policy $\\pi_{\\theta}$ (here we let $\\theta$ denote the parameters of the policy $\\pi$)--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "max_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize $J$ using gradient ascent, we need an efficiently computable form of its gradient. First, let's compute an analytical form of the gradient, then see how we can approximate it with sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-derivative trick to push a form of the gradient of the policy into the the gradient of $J$.\n",
    "\n",
    "I liked Daniel Takeshi's explanation of it--\"the log derivative trick tells us how to insert a log into an expectation when starting from $\\nabla_{\\theta} \\mathbb{E}[f(x)]$\"\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}[f(x)] &= \\nabla_{\\theta} \\int p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int \\dfrac{p_{\\theta}(x)}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int p_{\\theta}(x) \\nabla_{\\theta} \\log p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\mathbb{E}[\\nabla_{\\theta} \\log p_{\\theta}(x) f(x)]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gradient of $J$ we are concerned with the gradient of the log probability of trajectories, so we derive its form now.\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(\\tau) &= \\nabla_{\\theta} \\log \\left( \\mu(s_{0}) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t|s_t) P(s_{t+1|s_t,a_t}) \\right) \\\\\n",
    "&= \\nabla_{\\theta} \\left( \\log \\mu_{s_0} + \\sum_{t=0}^{T-1} (\\log \\pi_{\\theta}(a_t | s_t) + \\log P(s_{t+1} | s_t, a_t))\\right) \\\\\n",
    "&= \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that the dynamics of the environment $P$ disappears, as it does not depend on $\\theta$, which shows that policy gradients can be used in a model-free manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the pieces together, the gradient of the expected return is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log probability of each action is weighted by the rewards associated with it. The gradient has an intuitive interpretation--it encourages us to increase the probability of actions which lead to high expected return.\n",
    "\n",
    "Conveniently, $\\nabla_{\\theta} J$ turns out to have the form of an expectation. Because of this, as long as we can take the gradient of the log of our policy, we can estimate it with Monte Carlo simulation \\[link\\] using samples from our environment--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{g} = \\dfrac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal{D}$ is a dataset of trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance ###\n",
    "\n",
    "An issue with the estimator $\\hat{g}$ is that it has high variance. One of the reasons for this is that $R(\\tau)$ sums up individual reward samples over long sequences. \n",
    "\n",
    "To see this, let $R_t = \\sum_{i=0}^t r_t$, the sum of rewards up to time step $t$. Then in general, a longer trajectory has a higher variance in cumulative rewards than a shorter trajectory, since rewards are correlated within an episode. (Intuitively, the actions you take influence the distribution of future states you will see, and also the actions further available to you.) To formally see this, \n",
    "\n",
    "\\begin{align*}\n",
    "Var(R_{t+1}) &= \\sum_{i=1}^{t+1} Var(r_i) + \\sum_{i \\ne j} Cov(r_i, r_j) \\\\\n",
    "            &= Var(R_t) + Var(r_{t+1}) + \\sum_{i=0}^t Cov(r_i, r_{t+1}) \\\\\n",
    "            & \\ge  Var(R_t) \\\\\n",
    "\\end{align*}\n",
    "(as long as $\\sum_{i=0}^t Cov(r_i, r_{t+1})$ is non-negative.)\n",
    "\n",
    "If we have high variance in our estimate of the policy gradient, then the update will cause the policy to fluctuate across updates.\n",
    "\n",
    "One way to decrease variance of the estimator starts by observing that the action taken at time $t$ affects only the rewards reaped at timestep $t$ onwards. This makes sense in terms of credit assignment, and also reduces the number of timesteps we sum over. Let's do some algebra to get the gradient into a form we want:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R (\\tau)] &= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t' = 0}^T r_{t'} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})]  \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The literature likes to refer to $\\sum_{t' = t}^T r_{t'}$ as rewards-to-go. We weight each action its causal consequence--the future rewards it impacts. We note that this form has a resemblance to Bellman forms of value, which concern the reward at a given time step, as well as the future consequences.\n",
    "\n",
    "A second way to decrease the variance of the policy gradient estimate is by subtracting a baseline from the rewards-to-go. \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'} - b(s_t))]\n",
    "\\end{equation}\n",
    "\n",
    "An intuitive choice for $b(s_t)$ is the state-value function $V^{\\pi}(s_t)$. When we use the state-value function as a baseline, we are choosing to weight an action by the difference between the rewards we got in our sample and the rewards we expected to get. If the difference is close to zero, then we shouldn't need to change our policy too much--i.e. the gradient for that state-action should be close to zero in the update.\n",
    "\n",
    "There are a few ways to more formally explain why the baseline reduces the variance of the policy gradient estimator. I'll first summarize Daniel Takeshi's explanation, which is an approximate one.\n",
    "\n",
    "\\begin{align*}\n",
    "Var(\\hat{g}) &= Var(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t)) \\\\\n",
    "&  \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t))^2] \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[(\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He first approximates the variance of a sum by the sum over timesteps of variances. Then he uses the formula $Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ noting that $\\mathbb{E}[X] = 0$ for the log probability form. The third approximation comes from independence. Daniel Takeshi says that the approximations are fine since recent advances in RL, like A3C, break up correlation among samples.\n",
    "\n",
    "Based on these approximations, we can see that that a well-chosen baseline can decrease the variance of $\\hat{g}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network ###\n",
    "\n",
    "The policy is represented as a multi-layer perceptron, since we are interested in acting in high dimensional state spaces with continuous actions. (On an aside, a less utilitarian and very beautiful thing about deep RL is that it connects neuroscience with artificial intelligence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, size):    \n",
    "    modules = OrderedDict()\n",
    "    modules['Linear_Input'] = nn.Linear(input_size, size)\n",
    "    modules['ReLU_Input'] = nn.ReLU()\n",
    "    for i in range(n_layers):\n",
    "        modules['Linear_'+str(i)] = nn.Linear(size, size)\n",
    "        modules['ReLU_'+str(i)] = nn.ReLU()\n",
    "    modules['Linear_Output'] = nn.Linear(size,output_size)\n",
    "    sequential = nn.Sequential(modules)\n",
    "    return sequential\n",
    "\n",
    "# cartpole\n",
    "observation_dim        = 4\n",
    "n_layers               = 1\n",
    "layer_size             = 64\n",
    "\n",
    "policy_network = build_mlp(observation_dim, 1, n_layers, layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss ###\n",
    "\n",
    "Below is the implementation of the policy gradient update. Note here that advantage denotes $\\sum_{t' = t}^T r_{t'}- V^{\\pi}(s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_8284323280569314814() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_8284323280569314814()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.chdir(\"/Users/annhe/vpg/code\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code/policy_gradient.py\n",
    "\n",
    "def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        actions = np2torch(actions)\n",
    "        advantages = np2torch(advantages)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        res = self.policy.action_distribution(observations).log_prob(actions) \n",
    "\n",
    "        loss = -(res * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Refitting ###\n",
    "\n",
    "Note that we will have to update the baseline, $V^{\\pi}(s_t)$ every time we perform a policy update. This is because we want our estimate of the value function to be as accurate as possible, given the data we have. To do so, we compute the mean squared error loss between a forward pass of the network (current value estimate) and the most recent returns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size], containing all discounted\n",
    "                future returns for each step\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        \"\"\"\n",
    "        returns = np2torch(returns)\n",
    "        observations = np2torch(observations)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.forward(observations)\n",
    "        loss = ((returns - preds)**2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###\n",
    "\n",
    "Now we are ready to run some experiments. We would hope to see that the introduction of the baseline improves performance over just using the returns in the policy gradient estimate. To do so, we experiment on three OpenAI gym environments--cartpole, pendulum, and half-cheetah. For each environment we choose three random seeds, and run the agent with and without baseline using those three seeds. We plot the average expected return for each experiment with error bars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cartpole and pendulum results do not really demonstrate the difference between baseline and no baseline, but cheetah seems to be a complex enough environment to show the difference. In cheetah, we see that the baseline approach outperforms no baseline, especially at the very end, where the no baseline approach actual deteriorates in performance while the baseline approach continues to improve. Without a baseline, the policy gradient updates may be too large. In fact, even with a baseline, VPG is too \"greedy\" an approach. An intuitive explanation is that overcorrecting (changing the way you act too drastically) based on feedback may incur immediate benefits but prevent long term learning and generalization. Follow-up work like TRPO and PPO aim to make monotonic improvements in performance by constraining the difference between the policy before and after updates using KL-Divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Results / Baseline vs no Baseline ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole-old.png\">\n",
    "<img src=\"code/results/results-pendulum-old.png\">\n",
    "<img src=\"code/results/results-cheetah-old.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Improvement and TRPO ###\n",
    "\n",
    "(this section is highly based off of John Schulman's PhD thesis.)\n",
    "\n",
    "As shown in the experiment above, VPG does not necessarily make monotonic improvements to the expected performance of a policy. TRPO constrains the step-size of the gradient updates to impose monotonic improvement. First we derive the theoretical basis for TRPO, then we look into the details and make approximations and estimates in order to implement it.\n",
    "\n",
    "Instead of setting the optimization objective to be the expected performance of a policy, we instead set the objective to be a loss function whose monotonic improvement guaranteed monotonic improve of the policy itself. Consider the following identity due to Kakade and Langford,\n",
    "\n",
    "\\begin{equation}\n",
    "\\eta_{\\pi}(\\tilde{\\pi}) = \\eta_{\\pi}(\\pi) + \\mathbb{E}_{\\tau \\sim \\tilde{\\pi}}[\\sum_{t=0}^{\\infty} \\gamma^{t} A_{\\pi}(s_{t}, a_{t})]\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\eta_{\\pi}(\\pi)$ is the discounted sum of rewards from following $\\pi$ and $A_{\\pi}(s_{t}, a_{t})$ is the advantage of taking action $a_{t}$ over following $\\pi$ at time step $t$. If we massage this equation into a slightly different form, we can begin to see the theoretical basis for TRPO.\n",
    "\n",
    "Note that $\\mathbb{E}_{\\tau \\sim \\tilde{\\pi}} = \\mathbb{E}_{s_0, a_0, s_1, a_1, ... \\sim \\tilde{\\pi}}$.\n",
    "\n",
    "If we define $P(s_t = s | \\tilde{\\pi})$ to be the probability that we end up in state $s$ at time $t$ by following $\\pi$, then we can conveniently rewrite $\\eta_{\\pi}(\\tilde{\\pi})$ as \n",
    "\n",
    "\\begin{align*}\n",
    "\\eta_{\\pi}(\\tilde{\\pi}) &= \\eta_{\\pi}(\\pi) + \\mathbb{E}_{s_0, a_0, s_1, a_1, ... \\sim \\tilde{\\pi}}[\\sum_{t=0}^{\\infty} \\gamma^{t} A_{\\pi}(s_{t}, a_{t})] \\\\\n",
    "&= \\eta_{\\pi}(\\pi) + \\sum_{t=0}^{\\infty} \\sum_{s} P(s_t = s | \\tilde{\\pi}) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a) \\\\\n",
    "&= \\eta_{\\pi}(\\pi) +  \\sum_{s} \\sum_{t=0}^{\\infty} P(s_t = s | \\tilde{\\pi}) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a) \\\\\n",
    "&= \\eta_{\\pi}(\\pi) +  \\sum_{s} p_{\\tilde{\\pi}}(s) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "Where $p_{\\tilde{\\pi}}(s) = P(s_0 = s | \\tilde{\\pi}) + \\gamma \\cdot P(s_1 = s | \\tilde{\\pi}) + ...$ is the discounted state visitation frequency. From the formulation above, we can see that any policy update that has positive expected advantage at every state is guaranteed to improve policy performance.\n",
    "\n",
    "However, in our computation of the policy update, we cannot compute $p_{\\tilde{\\pi}}(s)$ as we do not have samples from the new policy $\\tilde{\\pi}$. We make a local approximation to $\\eta$ by using $p_{\\pi}(s)$ instead, yielding the modified objective\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\pi}(\\tilde{\\pi}) = \\eta_{\\pi}(\\pi) +  \\sum_{s} p_{\\pi}(s) \\sum_{a} \\tilde{\\pi}(a|s) \\gamma^{t} \\mathcal{A}_{\\pi}(s,a)\n",
    "\\end{equation}\n",
    "\n",
    "The first order gradient of $L_{\\pi}(\\tilde{\\pi})$ and $\\eta_{\\pi}(\\tilde{\\pi})$ are equivalent, which means we can indirectly improve $\\eta_{\\pi}(\\tilde{\\pi})$ by directly optimizing $L_{\\pi}(\\tilde{\\pi})$. However, we do not yet know how large a step size we can take and still have guaranteed improvement.\n",
    "\n",
    "A theorem (cite) says\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi_{new}} \\ge L_{\\pi_{old}}(\\pi_{new}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} \\left( D_{TV}^{max}(\\pi_{old}, \\pi_{new}) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "Since $(D_{TV})^2$ is upper bounded by KL divergence, we also get\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi_{new}} \\ge L_{\\pi_{old}}(\\pi_{new}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{old}, \\pi_{new}) \n",
    "\\end{equation}\n",
    "\n",
    "To see that we can get monotonic improvement by optimizing $L$,\n",
    "\n",
    "Let $M_i(\\pi) = L_{\\pi_i}(\\pi) - \\dfrac{4 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{old}, \\pi_{new})$\n",
    "\n",
    "Note that $M_i(\\pi_i) = L_{\\pi_i}(\\pi_i) =  V^{\\pi_i}$\n",
    "\n",
    "Then \n",
    "\n",
    "\\begin{align*}\n",
    "V^{\\pi_{i+1}} & \\ge L_{\\pi_{i}}(\\pi_{i+1}) - \\dfrac{2 \\epsilon \\gamma}{(1-\\gamma)^2} D_{KL}^{max}(\\pi_{i}, \\pi_{i+1}) \\\\\n",
    "\\end{align*}\n",
    "And subtracting $V^{\\pi_{i}}$ from both sides\n",
    "\\begin{align*}\n",
    "V^{\\pi_{i+1}} - V^{\\pi_{i}} & \\ge M_{i}(\\pi_{i+1}) - M_{i}(\\pi_{i})\\\\\n",
    "\\end{align*}\n",
    "Since $M_i(\\pi_i) =  V^{\\pi_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of optimizing $M$, which includes a penalty term for the KL divergence, TRPO optimizes against a constraint on the KL divergence. There are sample-based ways to estimate the objective and constraint, which you can read about in John's PhD thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO ###\n",
    "\n",
    "Since TRPO uses a second-order optimization technique (conjugate gradient descent) to optimize the objective, we turn our focus on a simpler but similar algorithm, PPO. PPO is still motivated by the question of making monotonic improvements to a policy, but uses numerical methods which compute only first derivatives.\n",
    "\n",
    "Here, we focus on PPO-Clip, which replaces the KL-divergence term by clipping the objective function to remove incentives to make big changes to the policy. \n",
    "\n",
    "The PPO objective function is\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{k+1} = arg max_{\\theta} \\mathbb{E}_{s,a \\sim \\pi_{\\theta_{k}}} [L(s,a,\\theta_{k}, \\theta)]\n",
    "\\end{equation}\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(s,a,\\theta_{k}, \\theta) = min \\big( \\dfrac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)} A^{\\pi_{\\theta_{k}}}(s,a), g(\\epsilon,A^{\\pi_{\\theta_{k}}}(s,a)) \\big)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "g(\\epsilon,A^{\\pi_{\\theta_{k}}}(s,a)) = clip \\big(\\dfrac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}, 1-\\epsilon, 1+\\epsilon \\big)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the clipping term as analogous to KL-constraining the old and new policies in TRPO. Taking the of min the clipped objective and unclipped objective \\[cite PPO paper\\] corresponds to taking a pessimistic (i.e. lower) bound on the original object. In our implementation of PPO-clip, we set $\\epsilon=0.2$, which was shown in the PPO paper to lead to the best average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO vs VPG (baseline) ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole-ppo-vpg.png\">\n",
    "<img src=\"code/results/results-pendulum-ppo-vpg.png\">\n",
    "<img src=\"code/results/results-cheetah-ppo-vpg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment in cartpole most clearly shows the advantages of PPO over VPG. \n",
    "\n",
    "- note that it's not monotonic... why?\n",
    "- note about hyper-parameters: batch size, step size, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
