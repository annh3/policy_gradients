{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "knowing-cause",
   "metadata": {},
   "source": [
    "### Preliminary Notes ###\n",
    "\n",
    "High level themes\n",
    "\n",
    "* Alternating optimization (separation of optimization into Q optimization and policy optimization)\n",
    "* Aka Actor-Critic methods in Reinforcement Learning\n",
    "* weight sharing\n",
    "* state generalization\n",
    "* Q: When does DDPG work and when does it fail?\n",
    "\n",
    "Medium level themes\n",
    "\n",
    "* off-policy data and Bellman equation to learn Q-function\n",
    "* Use q-functinon to learn policy\n",
    "\n",
    "Complexity issues (Learning the policy)\n",
    "\n",
    "* When there are a finite number of discrete actions, the max poses no problem\n",
    "* But when the action space is continuous, we need to differentiate\n",
    "* Using a normal optimization algorithm would make calculating max_a Q*(s,a) painfully expensive\n",
    "\n",
    "Assumption\n",
    "* Q*(s,a) is differentiable with respect to the action argument\n",
    "\n",
    "Then\n",
    "* Instead of computing max_a Q(s,a) we approximate it as max_a Q(s,a) = Q(s,mu(s))\n",
    "\n",
    "Policy Learning\n",
    "\\begin{equation}\n",
    "max_{\\theta} \\mathbb{E}[Q(s, \\mu(s))]\n",
    "\\end{equation}\n",
    "\n",
    "Q-Learning\n",
    "\n",
    "* Learn Q function using Bellman equation\n",
    "\\begin{equation}\n",
    "Q^{*}(s,a) = \\mathbb{E}_{s' \\sim P} [r(s,a) + \\gamma max_{a'} Q^{*}(s',a')]\n",
    "\\end{equation}\n",
    "\n",
    "* The bellman equation above is the starting point for learning an approximator to Q*(s,a)\n",
    "* S'pose we choose to approximate with an NN Q_{\\phi}(s,a) with parameters $\\phi$, and that we have collected the set $\\mathcal{D}$ of transitions $(s,a,r,s',d)$ where $d$ indicates whether state $s'$ is terminal\n",
    "* We can use the mean-squared Bellman error (MSBE) function to tell us how close $Q_{\\phi}$ is to satisfying the Bellman equation\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\phi, \\mathcal{D}) = \\mathbb{E}_{(s,a,r,s',d) \\sim \\mathcal{D}} \\big[ \\big( Q_{\\phi}(s,a) - (r + \\gamma(1-d)max_{a'} Q_{\\phi}(s',a') \\big)^2 \\big]\n",
    "\\end{equation}\n",
    "\n",
    "* When the state is done, we don't bootstrap into the future\n",
    "* Q-Leaning algorithms such as all variants of DQN and DDPG are largely based on this MSBE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-lindsay",
   "metadata": {},
   "source": [
    "### Technical Trick: Replay Buffer ###\n",
    "\n",
    "* Make use of previous examples\n",
    "* If you use only the most recent data, you will overfit to it, and things will break (recency bias)\n",
    "* If you use too much experience, you may slow down your learning\n",
    "* Q: What is the role of this in generalization error and alignment error?\n",
    "* Hebbian learning (inspiration from neuroscience/hippocampus)\n",
    "\n",
    "#### Technical details of Replay Buffer ####\n",
    "\n",
    "* We just need to be able to sample UAR!\n",
    "* How can we test? that it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-heath",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
