{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient ###\n",
    "\n",
    "In this notebook we will implement vanilla policy gradient (REINFORCE), a classic policy gradient reinforcement learning algorithm. The goal of reinforcement learning is for an agent to learn to act in a dynamic environment so as to maximize its expected cumulative reward over the course of a time-horizon. Policy gradient methods solve the problem of control by directly learning the policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ from observations of rewards obtained by interacting with the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally define a trajectory $\\tau$ as a tuple $(s_0, a_0, r_0, s_1, a_1, ..., r_T)$ denoting a sequence of state-action-rewards observed over the course of some episode of interaction with the agent's environment, and let $R(\\tau)$ denote the finite-horizon return, aka cumulative sum of rewards. Then our goal is the maximize the _expected_ finite-horizon return where the expectation is over trajectories sampled from the stochastic policy $\\pi_{\\theta}$ (here we let $\\theta$ denote the parameters of the policy $\\pi$)--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "max_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize $J$ using gradient ascent, we need a computable form of its gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the derivation for now, the gradient of the expected return is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log probability of each action is weighted by the rewards associated with it. The gradient has an intuitive interpretation--it encourages us to increase the probability of actions which lead to high expected return.\n",
    "\n",
    "Conveniently, $\\nabla_{\\theta} J$ turns out to have the form of an expectation. Because of this, we can estimate it using samples from our environment--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{g} = \\dfrac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal{D}$ is a dataset of trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network ###\n",
    "\n",
    "The policy is represented as a multi-layer perceptron so that we can learn to act in environments with high dimensional states and/or continous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, size):    \n",
    "    modules = OrderedDict()\n",
    "    modules['Linear_Input'] = nn.Linear(input_size, size)\n",
    "    modules['ReLU_Input'] = nn.ReLU()\n",
    "    for i in range(n_layers):\n",
    "        modules['Linear_'+str(i)] = nn.Linear(size, size)\n",
    "        modules['ReLU_'+str(i)] = nn.ReLU()\n",
    "    modules['Linear_Output'] = nn.Linear(size,output_size)\n",
    "    sequential = nn.Sequential(modules)\n",
    "    return sequential\n",
    "\n",
    "# cartpole\n",
    "observation_dim        = 4\n",
    "n_layers               = 1\n",
    "layer_size             = 64\n",
    "\n",
    "policy_network = build_mlp(observation_dim, 1, n_layers, layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/annhe/vpg/code\n",
      "/Users/annhe/vpg/code\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/Users/annhe/vpg/code\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load policy_gradient.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import os\n",
    "from general import get_logger, Progbar, export_plot\n",
    "from baseline_network import BaselineNetwork\n",
    "from network_utils import build_mlp, device, np2torch\n",
    "from policy import CategoricalPolicy, GaussianPolicy\n",
    "import pdb\n",
    "\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    \"\"\"\n",
    "    Class for implementing a policy gradient algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, env, config, seed, logger=None):\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient Class\n",
    "\n",
    "        Args:\n",
    "                env: an OpenAI Gym environment\n",
    "                config: class with hyperparameters\n",
    "                logger: logger instance from the logging module\n",
    "\n",
    "        You do not need to implement anything in this function. However,\n",
    "        you will need to use self.discrete, self.observation_dim,\n",
    "        self.action_dim, and self.lr in other methods.\n",
    "        \"\"\"\n",
    "        # directory for training outputs\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "        # store hyperparameters\n",
    "        self.config = config\n",
    "        self.seed = seed\n",
    "\n",
    "        self.logger = logger\n",
    "        if logger is None:\n",
    "            self.logger = get_logger(config.log_path)\n",
    "        self.env = env\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # discrete vs continuous action space\n",
    "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "\n",
    "        self.lr = self.config.learning_rate\n",
    "\n",
    "        self.init_policy()\n",
    "\n",
    "        if config.use_baseline:\n",
    "            self.baseline_network = BaselineNetwork(env, config)\n",
    "\n",
    "    def init_policy(self):\n",
    "        \"\"\"\n",
    "        Please do the following:\n",
    "        1. Create a network using build_mlp. It should map vectors of size\n",
    "           self.observation_dim to vectors of size self.action_dim, and use\n",
    "           the number of layers and layer size from self.config\n",
    "        2. If self.discrete = True (meaning that the actions are discrete, i.e.\n",
    "           from the set {0, 1, ..., N-1} where N is the number of actions),\n",
    "           instantiate a CategoricalPolicy.\n",
    "           If self.discrete = False (meaning that the actions are continuous,\n",
    "           i.e. elements of R^d where d is the dimension), instantiate a\n",
    "           GaussianPolicy. Either way, assign the policy to self.policy\n",
    "        3. Create an optimizer for the policy, with learning rate self.lr\n",
    "           Note that the policy is an instance of (a subclass of) nn.Module, so\n",
    "           you can call the parameters() method to get its parameters.\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 8-12 lines.   ############\n",
    "        self.network = build_mlp(self.observation_dim, self.action_dim, self.config.n_layers, self.config.layer_size)\n",
    "        if self.discrete:\n",
    "            self.policy = CategoricalPolicy(self.network)\n",
    "        else:\n",
    "            self.policy = GaussianPolicy(self.network, self.action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "    def init_averages(self):\n",
    "        \"\"\"\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "        self.avg_reward = 0.\n",
    "        self.max_reward = 0.\n",
    "        self.std_reward = 0.\n",
    "        self.eval_reward = 0.\n",
    "\n",
    "    def update_averages(self, rewards, scores_eval):\n",
    "        \"\"\"\n",
    "        Update the averages.\n",
    "        You don't have to change or use anything here.\n",
    "\n",
    "        Args:\n",
    "            rewards: deque\n",
    "            scores_eval: list\n",
    "        \"\"\"\n",
    "        self.avg_reward = np.mean(rewards)\n",
    "        self.max_reward = np.max(rewards)\n",
    "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "\n",
    "        if len(scores_eval) > 0:\n",
    "            self.eval_reward = scores_eval[-1]\n",
    "\n",
    "    def record_summary(self, t):\n",
    "        pass\n",
    "\n",
    "    def sample_path(self, env, num_episodes = None):\n",
    "        \"\"\"\n",
    "        Sample paths (trajectories) from the environment.\n",
    "\n",
    "        Args:\n",
    "            num_episodes: the number of episodes to be sampled\n",
    "                if none, sample one batch (size indicated by config file)\n",
    "            env: open AI Gym envinronment\n",
    "\n",
    "        Returns:\n",
    "            paths: a list of paths. Each path in paths is a dictionary with\n",
    "                path[\"observation\"] a numpy array of ordered observations in the path\n",
    "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
    "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
    "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
    "\n",
    "        You do not have to implement anything in this function, but you will need to\n",
    "        understand what it returns, and it is worthwhile to look over the code\n",
    "        just so you understand how we are taking actions in the environment\n",
    "        and generating batches to train on.\n",
    "        \"\"\"\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "\n",
    "        while (num_episodes or t < self.config.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.config.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy.act(states[-1][None])[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "                if (done or step == self.config.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.config.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                    \"reward\" : np.array(rewards),\n",
    "                    \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        \"\"\"\n",
    "        Calculate the returns G_t for each timestep\n",
    "\n",
    "        Args:\n",
    "            paths: recorded sample paths. See sample_path() for details.\n",
    "\n",
    "        Return:\n",
    "            returns: return G_t for each timestep\n",
    "\n",
    "        After acting in the environment, we record the observations, actions, and\n",
    "        rewards. To get the advantages that we need for the policy update, we have\n",
    "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
    "        of Q^π (s_t, a_t):\n",
    "\n",
    "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
    "\n",
    "        where T is the last timestep of the episode.\n",
    "\n",
    "        Note that here we are creating a list of returns for each path\n",
    "\n",
    "        TODO: compute and return G_t for each timestep. Use self.config.gamma.\n",
    "        \"\"\"\n",
    "\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            #######################################################\n",
    "            #########   YOUR CODE HERE - 5-10 lines.   ############\n",
    "            rewards = rewards.tolist()\n",
    "            rewards.reverse()\n",
    "            returns = []\n",
    "            cur = rewards[0]\n",
    "            returns.append(cur)\n",
    "            for i, r in enumerate(rewards[1:]):\n",
    "                cur = r + self.config.gamma * cur\n",
    "                returns.append(cur)\n",
    "            \n",
    "            returns.reverse()\n",
    "            # remember to reverse again\n",
    "\n",
    "            #######################################################\n",
    "            #########          END YOUR CODE.          ############\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def normalize_advantage(self, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        Returns:\n",
    "            normalized_advantages: np.array of shape [batch size]\n",
    "\n",
    "        TODO:\n",
    "        Normalize the advantages so that they have a mean of 0 and standard\n",
    "        deviation of 1. Put the result in a variable called\n",
    "        normalized_advantages (which will be returned).\n",
    "\n",
    "        Note:\n",
    "        This function is called only if self.config.normalize_advantage is True.\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 1-2 lines.    ############\n",
    "        mean = np.mean(advantages)\n",
    "        std = np.std(advantages)\n",
    "        normalized_advantages = (advantages - mean) / std\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "        return normalized_advantages\n",
    "\n",
    "    def calculate_advantage(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Calculates the advantage for each of the observations\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size]\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        \"\"\"\n",
    "        if self.config.use_baseline:\n",
    "            # override the behavior of advantage by subtracting baseline\n",
    "            advantages = self.baseline_network.calculate_advantage(returns, observations)\n",
    "        else:\n",
    "            advantages = returns\n",
    "\n",
    "        if self.config.normalize_advantage:\n",
    "            advantages = self.normalize_advantage(advantages)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "\n",
    "        Note:\n",
    "        PyTorch optimizers will try to minimize the loss you compute, but you\n",
    "        want to maximize the policy's performance.\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        actions = np2torch(actions)\n",
    "        advantages = np2torch(advantages)\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 5-7 lines.    ############\n",
    "        self.optimizer.zero_grad()\n",
    "        res = self.policy.action_distribution(observations).log_prob(actions) \n",
    "        #res = self.policy.action_distribution(observations)\n",
    "        #pdb.set_trace()\n",
    "        loss = -(res * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs training\n",
    "\n",
    "        You do not have to change or use anything here, but take a look\n",
    "        to see how all the code you've written fits together!\n",
    "        \"\"\"\n",
    "        last_record = 0\n",
    "\n",
    "        self.init_averages()\n",
    "        all_total_rewards = []         # the returns of all episodes samples for training purposes\n",
    "        averaged_total_rewards = []    # the returns for each iteration\n",
    "\n",
    "        for t in range(self.config.num_batches):\n",
    "\n",
    "            # collect a minibatch of samples\n",
    "            paths, total_rewards = self.sample_path(self.env)\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "            # compute Q-val estimates (discounted future returns) for each time step\n",
    "            returns = self.get_returns(paths)\n",
    "\n",
    "            # advantage will depend on the baseline implementation\n",
    "            advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "            # run training operations\n",
    "            if self.config.use_baseline:\n",
    "                self.baseline_network.update_baseline(returns, observations)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "\n",
    "            # logging\n",
    "            if (t % self.config.summary_freq == 0):\n",
    "                self.update_averages(total_rewards, all_total_rewards)\n",
    "                self.record_summary(t)\n",
    "\n",
    "            # compute reward statistics for this batch and log\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            self.logger.info(msg)\n",
    "\n",
    "            if  self.config.record and (last_record > self.config.record_freq):\n",
    "                self.logger.info(\"Recording...\")\n",
    "                last_record = 0\n",
    "                self.record()\n",
    "\n",
    "        self.logger.info(\"- Training done.\")\n",
    "        np.save(self.config.scores_output, averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Score\", self.config.env_name, self.config.plot_output)\n",
    "\n",
    "    def evaluate(self, env=None, num_episodes=1):\n",
    "        \"\"\"\n",
    "        Evaluates the return for num_episodes episodes.\n",
    "        Not used right now, all evaluation statistics are computed during training\n",
    "        episodes.\n",
    "        \"\"\"\n",
    "        if env==None: env = self.env\n",
    "        paths, rewards = self.sample_path(env, num_episodes)\n",
    "        avg_reward = np.mean(rewards)\n",
    "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "        self.logger.info(msg)\n",
    "        return avg_reward\n",
    "\n",
    "    def record(self):\n",
    "        \"\"\"\n",
    "        Recreate an env and record a video for one episode\n",
    "        \"\"\"\n",
    "        env = gym.make(self.config.env_name)\n",
    "        env.seed(self.seed)\n",
    "        env = gym.wrappers.Monitor(env, self.config.record_path, video_callable=lambda x: True, resume=True)\n",
    "        self.evaluate(env, 1)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Apply procedures of training for a PG.\n",
    "        \"\"\"\n",
    "        # record one game at the beginning\n",
    "        if self.config.record:\n",
    "            self.record()\n",
    "        # model\n",
    "        self.train()\n",
    "        # record one game at the end\n",
    "        if self.config.record:\n",
    "            self.record()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Normalization ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Reasons for Variance Reduction ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Results / Baseline vs no Baseline ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole.png\">\n",
    "<img src=\"code/results/results-pendulum.png\">\n",
    "<img src=\"code/results/results-cheetah.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future -- Monotonic Improvement and TRPO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
