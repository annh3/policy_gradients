{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_7103591087767461646() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_7103591087767461646()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient ###\n",
    "\n",
    "In this notebook we will implement vanilla policy gradient (REINFORCE), a classic policy gradient reinforcement learning algorithm. Credits go to Seita's blog, John Schulman's notes, and the CS234 lectures, from which the explanations in this notebook are synthesized. I used the CS234 assignment 3 startercode to focus more on implementing the VPG logic and concepts over implementation parts like logging. \n",
    "\n",
    "The goal of reinforcement learning is for an agent to learn to act in a dynamic environment so as to maximize its expected cumulative reward over the course of a time-horizon. Policy gradient methods solve the problem of control by directly learning the policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$ from observations of rewards obtained by interacting with the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally define a trajectory $\\tau$ as a tuple $(s_0, a_0, r_0, s_1, a_1, ..., r_T)$ denoting a sequence of state-action-rewards observed over the course of some episode of interaction with the agent's environment, and let $R(\\tau)$ denote the finite-horizon return, aka cumulative sum of rewards. Then our goal is the maximize the _expected_ finite-horizon return where the expectation is over trajectories sampled from the stochastic policy $\\pi_{\\theta}$ (here we let $\\theta$ denote the parameters of the policy $\\pi$)--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "max_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize $J$ using gradient ascent, we need an efficiently computable form of its gradient. First, let's compute an analytical form of the gradient, then see how we can approximate it with sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the log-derivative trick to push a form of the gradient of the policy into the the gradient of $J$.\n",
    "\n",
    "I liked Seita's explanation of it--\"the log derivative trick tells us how to insert a log into an expectation when starting from $\\nabla_{\\theta} \\mathbb{E}[f(x)]$\"\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}[f(x)] &= \\nabla_{\\theta} \\int p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int \\dfrac{p_{\\theta}(x)}{p_{\\theta}(x)} \\nabla_{\\theta} p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\int p_{\\theta}(x) \\nabla_{\\theta} \\log p_{\\theta}(x) f(x) dx \\\\\n",
    "&= \\mathbb{E}[\\nabla_{\\theta} \\log p_{\\theta}(x) f(x)]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gradient of $J$ we are concerned with the gradient of the log probability of trajectories, so we derive its form now.\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(\\tau) &= \\nabla_{\\theta} \\log \\left( \\mu(s_{0}) \\prod_{t=0}^{T-1} \\pi_{\\theta}(a_t|s_t) P(s_{t+1|s_t,a_t}) \\right) \\\\\n",
    "&= \\nabla_{\\theta} \\left( \\log \\mu_{s_0} + \\sum_{t=0}^{T-1} (\\log \\pi_{\\theta}(a_t | s_t) + \\log P(s_{t+1} | s_t, a_t))\\right) \\\\\n",
    "&= \\nabla_{\\theta} \\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_t | s_t) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the pieces together, the gradient of the expected return is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log probability of each action is weighted by the rewards associated with it. The gradient has an intuitive interpretation--it encourages us to increase the probability of actions which lead to high expected return.\n",
    "\n",
    "Conveniently, $\\nabla_{\\theta} J$ turns out to have the form of an expectation. Because of this, we can estimate it a la Monte Carlo methods using samples from our environment--i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\hat{g} = \\dfrac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathcal{D}$ is a dataset of trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance ###\n",
    "\n",
    "An issue with the estimator $\\hat{g}$ is that it has high variance. (One of the reasons for this, which we will go into more depth later, is that $R(\\tau)$ sums up individual reward samples over long sequences.) If we have high variance in our estimate of the policy gradient, then the update will cause the policy to fluctuate across updates in ways that don't reflect the overall dynamics of the environment.\n",
    "\n",
    "One way to decrease variance of the estimator starts by observing that the action taken at time $t$ affects only the rewards reaped at timestep $t$ onwards. This makes sense in terms of credit assignment, and also reduces the number of timesteps we sum over. Let's do some algebra to get the gradient into a form we want\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R (\\tau)] &= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ R(\\tau) \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t' = 0}^T r_{t'} \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)] \\\\\n",
    "&= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})]  \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert explanation here for algebra]\n",
    "\n",
    "The literature likes to refer to $\\sum_{t' = t}^T r_{t'}$ as rewards-to-go. We weight each action its causal consequence--the future rewards it impacts. We note that this form has a resemblance to Bellman forms of value, which concern the reward at a given time step, as well as the future consequences.\n",
    "\n",
    "A second way to decrease the variance of the policy gradient estimate is by subtracting a baseline from the rewards-to-go. \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'} - b(s_t))]\n",
    "\\end{equation}\n",
    "\n",
    "An intuitive choice for $b(s_t)$ is the state-value function $V^{\\pi}(s_t)$. When we use the state-value function as a baseline, we are choosing to weight an action by the difference between the rewards we got in our sample and the rewards we expected to get. If the difference is close to zero, then we shouldn't need to change our policy too much--i.e. the gradient for that state-action should be close to zero in the update.\n",
    "\n",
    "There are a few ways to more formally explain why the baseline reduces the variance of the policy gradient estimator. I'll first summarize Seita's explanation, which is an approximate one.\n",
    "\n",
    "\\begin{align*}\n",
    "Var(\\hat{g}) &= Var(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t)) \\\\\n",
    "&  \\approx \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t) (\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "& \\approx \\sum_{t=0}^{T-1} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [(\\log \\pi_{\\theta}(a_t|s_t))^2] \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[(\\sum_{t' = t}^T r_{t'})- b(s_t))^2] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He first approximates the variance of a sum by the sum over timesteps of variances. Then he uses the formula $Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ noting that $\\mathbb{E}[X] = 0$ for the log probability form. [To-do: explanation and proof]. The third approximation comes from independence. Seita says that the approximations are fine since recent advances in RL, like A3C, break up correlation among samples.\n",
    "\n",
    "Based on these approximations, we can see that that a well-chosen baseline can decrease the variance of $\\hat{g}$.\n",
    "\n",
    "I'm unsatisfied with these explanations, and plan on adding my own based off of the CS234 assignment to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network ###\n",
    "\n",
    "The policy is represented as a multi-layer perceptron so that we can learn to act in environments with high dimensional states and/or continous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_mlp(input_size, output_size, n_layers, size):    \n",
    "    modules = OrderedDict()\n",
    "    modules['Linear_Input'] = nn.Linear(input_size, size)\n",
    "    modules['ReLU_Input'] = nn.ReLU()\n",
    "    for i in range(n_layers):\n",
    "        modules['Linear_'+str(i)] = nn.Linear(size, size)\n",
    "        modules['ReLU_'+str(i)] = nn.ReLU()\n",
    "    modules['Linear_Output'] = nn.Linear(size,output_size)\n",
    "    sequential = nn.Sequential(modules)\n",
    "    return sequential\n",
    "\n",
    "# cartpole\n",
    "observation_dim        = 4\n",
    "n_layers               = 1\n",
    "layer_size             = 64\n",
    "\n",
    "policy_network = build_mlp(observation_dim, 1, n_layers, layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss ###\n",
    "\n",
    "Below is the implementation of the policy gradient update. Note here that advantage denotes $\\sum_{t' = t}^T r_{t'}- V^{\\pi}(s_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_8910983649119327165() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_8910983649119327165()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "os.chdir(\"/Users/annhe/vpg/code\")\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from code/policy_gradient.py\n",
    "\n",
    "def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        actions = np2torch(actions)\n",
    "        advantages = np2torch(advantages)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        res = self.policy.action_distribution(observations).log_prob(actions) \n",
    "\n",
    "        loss = -(res * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Refitting ###\n",
    "\n",
    "Note that we will have to update the baseline, $V^{\\pi}(s_t)$ every time we perform a policy update. This is because we want our estimate of the value function to be as accurate as possible, given the data we have. To do so, we compute the mean squared error loss between a forward pass of the network (current value estimate) and the most recent returns we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_baseline(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size], containing all discounted\n",
    "                future returns for each step\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        \"\"\"\n",
    "        returns = np2torch(returns)\n",
    "        observations = np2torch(observations)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.forward(observations)\n",
    "        loss = ((returns - preds)**2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_6095733090095249607() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_6095733090095249607()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load baseline_network.py (ALICE SKIP THIS!!!)\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network_utils import build_mlp, device, np2torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class BaselineNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for implementing Baseline network\n",
    "    \"\"\"\n",
    "    def __init__(self, env, config):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Create self.network using build_mlp, and create self.optimizer to\n",
    "        optimize its parameters.\n",
    "        You should find some values in the config, such as the number of layers,\n",
    "        the size of the layers, and the learning rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.env = env\n",
    "        self.baseline = None\n",
    "        self.lr = self.config.learning_rate\n",
    "\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 2-8 lines.   #############\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.network = build_mlp(self.observation_dim, 1, self.config.n_layers, self.config.layer_size)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = self.lr)\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = OrderedDict()\n",
    "        for name, param in self.network.named_parameters():\n",
    "            weights[name] = param.detach().numpy()\n",
    "        return weights\n",
    "\n",
    "    def calculate_deltas(self, weights_1, weights_2):\n",
    "        deltas = OrderedDict()\n",
    "        for name in weights_1:\n",
    "            delta = np.linalg.norm(weights_1[name] - weights_2[name])\n",
    "            deltas[name] = delta\n",
    "        return deltas\n",
    "\n",
    "    def print_weights(self):\n",
    "        return\n",
    "        # for param in self.network.parameters():\n",
    "        #     print(param)\n",
    "    def print_params(self):\n",
    "        for name, param in self.network.named_parameters():\n",
    "            print(name)\n",
    "            print(param.requires_grad)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: torch.Tensor of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            output: torch.Tensor of shape [batch size]\n",
    "\n",
    "        TODO:\n",
    "        Run the network forward and then squeeze the result so that it's\n",
    "        1-dimensional. Put the squeezed result in a variable called \"output\"\n",
    "        (which will be returned).\n",
    "\n",
    "        Note:\n",
    "        A nn.Module's forward method will be invoked if you\n",
    "        call it like a function, e.g. self(x) will call self.forward(x).\n",
    "        When implementing other methods, you should use this instead of\n",
    "        directly referencing the network (so that the shape is correct).\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 1 lines.     #############\n",
    "        output = torch.squeeze(self.network(observations))\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "        assert output.ndim == 1\n",
    "        return output\n",
    "\n",
    "    def calculate_advantage(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size]\n",
    "                all discounted future returns for each step\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        TODO:\n",
    "        Evaluate the baseline and use the result to compute the advantages.\n",
    "        Put the advantages in a variable called \"advantages\" (which will be\n",
    "        returned).\n",
    "\n",
    "        Note:\n",
    "        The arguments and return value are numpy arrays. The np2torch function\n",
    "        converts numpy arrays to torch tensors. You will have to convert the\n",
    "        network output back to numpy, which can be done via the numpy() method.\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 1-4 lines.   ############\n",
    "        res = self.forward(observations)\n",
    "        res = res.detach().numpy()\n",
    "        advantages = returns - res\n",
    "        assert(isinstance(advantages, np.ndarray))\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "        return advantages\n",
    "\n",
    "    def update_baseline(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size], containing all discounted\n",
    "                future returns for each step\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "\n",
    "        TODO:\n",
    "        Compute the loss (MSE), backpropagate, and step self.optimizer.\n",
    "        You may (though not necessary) find it useful to do perform these steps\n",
    "        more than one once, since this method is only called once per policy update.\n",
    "        If you want to use mini-batch SGD, we have provided a helper function\n",
    "        called batch_iterator (implemented in general.py).\n",
    "        \"\"\"\n",
    "        returns = np2torch(returns)\n",
    "        observations = np2torch(observations)\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 4-10 lines.  #############\n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.forward(observations)\n",
    "        loss = ((returns - preds)**2).mean()\n",
    "        loss.backward()\n",
    "        #print(\"Loss: \", loss)\n",
    "        self.optimizer.step()\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell",
     "hidecode"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_16188522000980874923() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_16188522000980874923()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load policy_gradient.py (ALICE SKIP THIS!!!)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import os\n",
    "from general import get_logger, Progbar, export_plot\n",
    "from baseline_network import BaselineNetwork\n",
    "from network_utils import build_mlp, device, np2torch\n",
    "from policy import CategoricalPolicy, GaussianPolicy\n",
    "import pdb\n",
    "\n",
    "\n",
    "class PolicyGradient(object):\n",
    "    \"\"\"\n",
    "    Class for implementing a policy gradient algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, env, config, seed, logger=None):\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient Class\n",
    "\n",
    "        Args:\n",
    "                env: an OpenAI Gym environment\n",
    "                config: class with hyperparameters\n",
    "                logger: logger instance from the logging module\n",
    "\n",
    "        You do not need to implement anything in this function. However,\n",
    "        you will need to use self.discrete, self.observation_dim,\n",
    "        self.action_dim, and self.lr in other methods.\n",
    "        \"\"\"\n",
    "        # directory for training outputs\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "        # store hyperparameters\n",
    "        self.config = config\n",
    "        self.seed = seed\n",
    "\n",
    "        self.logger = logger\n",
    "        if logger is None:\n",
    "            self.logger = get_logger(config.log_path)\n",
    "        self.env = env\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # discrete vs continuous action space\n",
    "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "\n",
    "        self.lr = self.config.learning_rate\n",
    "\n",
    "        self.init_policy()\n",
    "\n",
    "        if config.use_baseline:\n",
    "            self.baseline_network = BaselineNetwork(env, config)\n",
    "\n",
    "    def init_policy(self):\n",
    "        \"\"\"\n",
    "        Please do the following:\n",
    "        1. Create a network using build_mlp. It should map vectors of size\n",
    "           self.observation_dim to vectors of size self.action_dim, and use\n",
    "           the number of layers and layer size from self.config\n",
    "        2. If self.discrete = True (meaning that the actions are discrete, i.e.\n",
    "           from the set {0, 1, ..., N-1} where N is the number of actions),\n",
    "           instantiate a CategoricalPolicy.\n",
    "           If self.discrete = False (meaning that the actions are continuous,\n",
    "           i.e. elements of R^d where d is the dimension), instantiate a\n",
    "           GaussianPolicy. Either way, assign the policy to self.policy\n",
    "        3. Create an optimizer for the policy, with learning rate self.lr\n",
    "           Note that the policy is an instance of (a subclass of) nn.Module, so\n",
    "           you can call the parameters() method to get its parameters.\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 8-12 lines.   ############\n",
    "        self.network = build_mlp(self.observation_dim, self.action_dim, self.config.n_layers, self.config.layer_size)\n",
    "        if self.discrete:\n",
    "            self.policy = CategoricalPolicy(self.network)\n",
    "        else:\n",
    "            self.policy = GaussianPolicy(self.network, self.action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "    def init_averages(self):\n",
    "        \"\"\"\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "        self.avg_reward = 0.\n",
    "        self.max_reward = 0.\n",
    "        self.std_reward = 0.\n",
    "        self.eval_reward = 0.\n",
    "\n",
    "    def update_averages(self, rewards, scores_eval):\n",
    "        \"\"\"\n",
    "        Update the averages.\n",
    "        You don't have to change or use anything here.\n",
    "\n",
    "        Args:\n",
    "            rewards: deque\n",
    "            scores_eval: list\n",
    "        \"\"\"\n",
    "        self.avg_reward = np.mean(rewards)\n",
    "        self.max_reward = np.max(rewards)\n",
    "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "\n",
    "        if len(scores_eval) > 0:\n",
    "            self.eval_reward = scores_eval[-1]\n",
    "\n",
    "    def record_summary(self, t):\n",
    "        pass\n",
    "\n",
    "    def sample_path(self, env, num_episodes = None):\n",
    "        \"\"\"\n",
    "        Sample paths (trajectories) from the environment.\n",
    "\n",
    "        Args:\n",
    "            num_episodes: the number of episodes to be sampled\n",
    "                if none, sample one batch (size indicated by config file)\n",
    "            env: open AI Gym envinronment\n",
    "\n",
    "        Returns:\n",
    "            paths: a list of paths. Each path in paths is a dictionary with\n",
    "                path[\"observation\"] a numpy array of ordered observations in the path\n",
    "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
    "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
    "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
    "\n",
    "        You do not have to implement anything in this function, but you will need to\n",
    "        understand what it returns, and it is worthwhile to look over the code\n",
    "        just so you understand how we are taking actions in the environment\n",
    "        and generating batches to train on.\n",
    "        \"\"\"\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "\n",
    "        while (num_episodes or t < self.config.batch_size):\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.config.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy.act(states[-1][None])[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "                if (done or step == self.config.max_ep_len-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.config.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                    \"reward\" : np.array(rewards),\n",
    "                    \"action\" : np.array(actions)}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        \"\"\"\n",
    "        Calculate the returns G_t for each timestep\n",
    "\n",
    "        Args:\n",
    "            paths: recorded sample paths. See sample_path() for details.\n",
    "\n",
    "        Return:\n",
    "            returns: return G_t for each timestep\n",
    "\n",
    "        After acting in the environment, we record the observations, actions, and\n",
    "        rewards. To get the advantages that we need for the policy update, we have\n",
    "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
    "        of Q^π (s_t, a_t):\n",
    "\n",
    "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
    "\n",
    "        where T is the last timestep of the episode.\n",
    "\n",
    "        Note that here we are creating a list of returns for each path\n",
    "\n",
    "        TODO: compute and return G_t for each timestep. Use self.config.gamma.\n",
    "        \"\"\"\n",
    "\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            #######################################################\n",
    "            #########   YOUR CODE HERE - 5-10 lines.   ############\n",
    "            rewards = rewards.tolist()\n",
    "            rewards.reverse()\n",
    "            returns = []\n",
    "            cur = rewards[0]\n",
    "            returns.append(cur)\n",
    "            for i, r in enumerate(rewards[1:]):\n",
    "                cur = r + self.config.gamma * cur\n",
    "                returns.append(cur)\n",
    "            \n",
    "            returns.reverse()\n",
    "            # remember to reverse again\n",
    "\n",
    "            #######################################################\n",
    "            #########          END YOUR CODE.          ############\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def normalize_advantage(self, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        Returns:\n",
    "            normalized_advantages: np.array of shape [batch size]\n",
    "\n",
    "        TODO:\n",
    "        Normalize the advantages so that they have a mean of 0 and standard\n",
    "        deviation of 1. Put the result in a variable called\n",
    "        normalized_advantages (which will be returned).\n",
    "\n",
    "        Note:\n",
    "        This function is called only if self.config.normalize_advantage is True.\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 1-2 lines.    ############\n",
    "        mean = np.mean(advantages)\n",
    "        std = np.std(advantages)\n",
    "        normalized_advantages = (advantages - mean) / std\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "        return normalized_advantages\n",
    "\n",
    "    def calculate_advantage(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Calculates the advantage for each of the observations\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size]\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        \"\"\"\n",
    "        if self.config.use_baseline:\n",
    "            # override the behavior of advantage by subtracting baseline\n",
    "            advantages = self.baseline_network.calculate_advantage(returns, observations)\n",
    "        else:\n",
    "            advantages = returns\n",
    "\n",
    "        if self.config.normalize_advantage:\n",
    "            advantages = self.normalize_advantage(advantages)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "\n",
    "        Note:\n",
    "        PyTorch optimizers will try to minimize the loss you compute, but you\n",
    "        want to maximize the policy's performance.\n",
    "        \"\"\"\n",
    "        observations = np2torch(observations)\n",
    "        actions = np2torch(actions)\n",
    "        advantages = np2torch(advantages)\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 5-7 lines.    ############\n",
    "        self.optimizer.zero_grad()\n",
    "        res = self.policy.action_distribution(observations).log_prob(actions) \n",
    "        #res = self.policy.action_distribution(observations)\n",
    "        #pdb.set_trace()\n",
    "        loss = -(res * advantages).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #######################################################\n",
    "        #########          END YOUR CODE.          ############\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs training\n",
    "\n",
    "        You do not have to change or use anything here, but take a look\n",
    "        to see how all the code you've written fits together!\n",
    "        \"\"\"\n",
    "        last_record = 0\n",
    "\n",
    "        self.init_averages()\n",
    "        all_total_rewards = []         # the returns of all episodes samples for training purposes\n",
    "        averaged_total_rewards = []    # the returns for each iteration\n",
    "\n",
    "        for t in range(self.config.num_batches):\n",
    "\n",
    "            # collect a minibatch of samples\n",
    "            paths, total_rewards = self.sample_path(self.env)\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "            # compute Q-val estimates (discounted future returns) for each time step\n",
    "            returns = self.get_returns(paths)\n",
    "\n",
    "            # advantage will depend on the baseline implementation\n",
    "            advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "            # run training operations\n",
    "            if self.config.use_baseline:\n",
    "                self.baseline_network.update_baseline(returns, observations)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "\n",
    "            # logging\n",
    "            if (t % self.config.summary_freq == 0):\n",
    "                self.update_averages(total_rewards, all_total_rewards)\n",
    "                self.record_summary(t)\n",
    "\n",
    "            # compute reward statistics for this batch and log\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            self.logger.info(msg)\n",
    "\n",
    "            if  self.config.record and (last_record > self.config.record_freq):\n",
    "                self.logger.info(\"Recording...\")\n",
    "                last_record = 0\n",
    "                self.record()\n",
    "\n",
    "        self.logger.info(\"- Training done.\")\n",
    "        np.save(self.config.scores_output, averaged_total_rewards)\n",
    "        export_plot(averaged_total_rewards, \"Score\", self.config.env_name, self.config.plot_output)\n",
    "\n",
    "    def evaluate(self, env=None, num_episodes=1):\n",
    "        \"\"\"\n",
    "        Evaluates the return for num_episodes episodes.\n",
    "        Not used right now, all evaluation statistics are computed during training\n",
    "        episodes.\n",
    "        \"\"\"\n",
    "        if env==None: env = self.env\n",
    "        paths, rewards = self.sample_path(env, num_episodes)\n",
    "        avg_reward = np.mean(rewards)\n",
    "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "        self.logger.info(msg)\n",
    "        return avg_reward\n",
    "\n",
    "    def record(self):\n",
    "        \"\"\"\n",
    "        Recreate an env and record a video for one episode\n",
    "        \"\"\"\n",
    "        env = gym.make(self.config.env_name)\n",
    "        env.seed(self.seed)\n",
    "        env = gym.wrappers.Monitor(env, self.config.record_path, video_callable=lambda x: True, resume=True)\n",
    "        self.evaluate(env, 1)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Apply procedures of training for a PG.\n",
    "        \"\"\"\n",
    "        # record one game at the beginning\n",
    "        if self.config.record:\n",
    "            self.record()\n",
    "        # model\n",
    "        self.train()\n",
    "        # record one game at the end\n",
    "        if self.config.record:\n",
    "            self.record()\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment ###\n",
    "\n",
    "Now we are ready to run some experiments. We would hope to see that the introduction of the baseline improves performance over just using the returns in the policy gradient estimate. To do so, we experiment on three OpenAI gym environments--cartpole, pendulum, and half-cheetah. For each environment we choose three random seeds, and run the agent with and without baseline using those three seeds. We plot the average expected return for each experiment with error bars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cartpole and pendulum results do not really demonstrate the difference between baseline and no baseline, but cheetah seems to be a complex enough environment to show the difference. In cheetah, we see that the baseline approach outperforms no baseline, especially at the very end, where the no baseline approach actual deteriorates in performance while the baseline approach continues to improve. Without a baseline, the policy gradient updates may be too large. In fact, even with a baseline, VPG is too \"greedy\" an approach. An intuitive explanation is that overcorrecting (changing the way you act too drastically) based on feedback may incur immediate benefits but prevent long term learning and generalization. Follow-up work like TRPO and PPO aim to make monotonic improvements in performance by constraining the difference between the policy before and after updates using KL-Divergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Results / Baseline vs no Baseline ###\n",
    "\n",
    "<img src=\"code/results/results-cartpole.png\">\n",
    "<img src=\"code/results/results-pendulum.png\">\n",
    "<img src=\"code/results/results-cheetah.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future -- Monotonic Improvement and TRPO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Normalization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
